#' @title Set up Google Cloud Build to run a targets pipeline
#' @export
#' @family Cloud Build functions
#' @description Creates a Google Cloud Build yaml file so as to execute \link[targets]{tar_make} pipelines
#'
#' Historical runs accumulate in the
#'   configured Google Cloud Storage bucket, and the latest output is downloaded before
#'   \link[targets]{tar_make} executes so up-to-date steps do not rerun.
#'
#' @details Steps to set up your target task in Cloud Build:
#'
#' \itemize{
#'   \item Create your `targets` workflow.
#'   \item Create a Dockerfile that holds the R and system dependencies for your workflow.  You can test the image using \link{cr_deploy_docker}.  Include \code{library(targets)} dependencies - a Docker image with \code{targets} installed is available at \code{gcr.io/gcer-public/targets}.
#'   \item Run \code{cr_build_targets} to create the cloudbuild yaml file.
#'   \item Run the build via \link{cr_build} or similar.  Each build should only recompute outdated targets.
#'   \item Optionally create a build trigger via \link{cr_buildtrigger}.
#'   \item Trigger a build. The first trigger will run the targets pipeline, subsequent runs will only recompute the outdated targets.
#'  }
#'
#' @section DAGs:
#'
#' If your target workflow has parallel processing steps then leaving this as default \code{cr_buildstep_targets_multi()} will create a build that uses waitFor and build ids to create a DAG.  Setting this to \code{cr_buildstep_targets_single()} will be single thread but you can then customise the \code{targets::tar_make} script.  Or add your own custom target buildsteps here using \link{cr_buildstep_targets}
#'
#' @return A Yaml object as generated by \link{cr_build_yaml}
#' @param path File path to write the Google Cloud Build yaml workflow file. Set to NULL to write no file and just return the \code{Yaml} object.
#' @param task_image An existing Docker image that will be used to run your targets workflow after the targets meta has been downloaded from Google Cloud Storage
#' @param target_folder Where target metadata will sit within the Google Cloud Storage bucket as a folder.  If NULL defaults to RStudio project name or "targets_cloudbuild" if no RStudio project found.
#' @param bucket The Google Cloud Storage bucket the target metadata will be saved to in folder `target_folder`
#' @param ... Other arguments passed to \link{cr_build_yaml}
#' @inheritDotParams cr_build_yaml
#' @param task_args A named list of additional arguments to send to \link{cr_buildstep_r()} when its executing the \link[targets]{tar_make()} command (such as environment arguments)
#' @param tar_make The R script that will run in the tar_make() step. Modify to include custom settings such as "script"
#' @param buildsteps Generated buildsteps that create the targets build
#'
#' @param ... Other arguments passed to \link{cr_build_yaml}
#' @inheritDotParams cr_build_yaml
#' @inheritParams cr_buildstep_targets
#' @inheritParams cr_buildstep_targets_setup
#' @inheritParams cr_buildstep_targets_teardown
#' @seealso \link{cr_buildstep_targets} if you want to customise the build
#' @examples
#'
#' cr_build_targets(path=tempfile())
cr_build_targets <- function(
  buildsteps = cr_buildstep_targets_multi(),
  path = "cloudbuild_targets.yaml",
  bucket = cr_bucket_get(),
  ...) {

  #checks here it is targets related?

  yaml <- cr_build_yaml(
    buildsteps,
    ...
  )

  if (!is.null(path)) cr_build_write(yaml, file = path)

  yaml
}

resolve_bucket_folder <- function(target_folder, bucket){
  if(is.null(target_folder)) {
    target_folder <- tryCatch(
      basename(rstudioapi::getActiveProject()),
      error = function(err){
        NULL
      }
    )
    if(is.null(target_folder)){
      target_folder <- "targets_cloudbuild"
    }
  }

  myMessage(sprintf("targets cloud location: gs://%s/%s",
                    bucket, target_folder),
            level = 3)

  # gs://bucket-name/target-folder
  sprintf("gs://%s/%s", bucket, target_folder)

}

#' Targets Cloud Build single threaded
#' @export
#' @rdname cr_build_targets
cr_buildstep_targets_single <- function(
  target_folder = NULL,
  bucket = cr_bucket_get(),
  task_image = "gcr.io/gcer-public/targets",
  task_args = list(),
  tar_make = "targets::tar_make()"
){

  target_bucket <- resolve_bucket_folder(target_folder,
                                         bucket)
  c(
    cr_buildstep_targets_setup(target_bucket),
    cr_buildstep_targets(task_args = task_args,
                         tar_make = tar_make,
                         task_image = task_image),
    cr_buildstep_targets_teardown(target_bucket)
  )
}

#' @rdname cr_build_targets
#' @export
#' @details
#'   Use \code{cr_build_targets_artifacts} to download the return values of a
#'   target Cloud Build, then \link[targets]{tar_read} to read the results.  You can set the downloaded files as the target store via \code{targets::tar_config_set(store="_targets_cloudbuild")}.  Set \code{download_folder = "_targets"} to overwrite your local targets store.
#' @inheritParams cr_build_artifacts
#' @param target_subfolder If you only want to download a specific folder from the _targets/ folder on Cloud Build then specify it here.
#' @return \code{cr_build_targets_artifacts} returns the file path to where the download occurred.
cr_build_targets_artifacts <- function(build,
                                       download_folder = "_targets_cloudbuild",
                                       target_subfolder = c("all", "meta", "objects", "user"),
                                       overwrite = TRUE) {
  target_subfolder <- match.arg(target_subfolder)

  bb <- build$source$storageSource$bucket
  if (is.null(bb)) {
    stop("Could not find bucket.  Is this not a build from cr_build_targets()?")
  }

  build_folder <- build$substitutions$`_TARGET_BUCKET`
  build_folder <- gsub(
    paste0("gs://", bb, "/"), "",
    build$substitutions$`_TARGET_BUCKET`
  )

  if (!nzchar(build_folder)) {
    stop("Could not find build folder in bucket.")
  }

  prefix <- build_folder
  if (target_subfolder != "all") {
    prefix <- paste0(prefix, "/", target_subfolder)
  }

  arts <- googleCloudStorageR::gcs_list_objects(
    bucket = bb, prefix = prefix
  )

  if (nrow(arts) == 0) {
    myMessage("No target build artifacts found")
    return(NULL)
  }

  # create targets folder structure
  dir.create(download_folder, showWarnings = FALSE)
  dir.create(file.path(download_folder, build_folder),
    showWarnings = FALSE
  )
  dir.create(file.path(download_folder, build_folder, "_targets"),
    showWarnings = FALSE
  )
  dir.create(file.path(download_folder, build_folder, "_targets", "meta"),
    showWarnings = FALSE
  )
  dir.create(file.path(download_folder, build_folder, "_targets", "objects"),
    showWarnings = FALSE
  )
  dir.create(file.path(download_folder, build_folder, "_targets", "user"),
    showWarnings = FALSE
  )

  withr::with_dir(
    download_folder,
    {
      lapply(arts$name, function(x) {
        googleCloudStorageR::gcs_get_object(x,
          bucket = bb,
          saveToDisk = x,
          overwrite = overwrite
        )
      })
    }
  )

  normalizePath(file.path(download_folder, build_folder))
}


#' Buildstep to run a targets pipeline on Cloud Build
#'
#' This is a buildstep to help upload a targets pipeline, see \link{cr_build_targets} for examples and suggested workflow
#' @export
#' @param task_args A named list of additional arguments to send to \link{cr_buildstep_r} when its executing the \link[targets]{tar_make} command (such as environment arguments or waitFor ids)
#' @param tar_make The R script that will run in the \code{tar_make()} step. Modify to include custom settings
#' @param task_image An existing Docker image that will be used to run your targets workflow after the targets meta has been downloaded from Google Cloud Storage
#' @family Cloud Buildsteps
cr_buildstep_targets <- function(
  task_args = list(),
  tar_make = "targets::tar_make()",
  task_image = "gcr.io/gcer-public/targets"){

  do.call(
    cr_buildstep_r,
    args = c(
      task_args,
      list(
        r = tar_make,
        name = task_image,
        id = "target pipeline"
      )
    )
  )

}

#' @export
#' @rdname cr_buildstep_targets
#' @param bucket The Google Cloud Storage bucket and folder the target metadata will be saved to, e.g. \code{gs://my-bucket/my_target_project}   You can also pass in build substitution variables such as \code{"${_MY_BUCKET}"}.
cr_buildstep_targets_setup <- function(bucket){
  cr_buildstep_bash(
    bash_script = paste(
      c("mkdir /workspace/_targets &&",
        "mkdir /workspace/_targets/meta &&",
        "gsutil -m cp -r",
        sprintf("%s/_targets/meta",bucket),
        "/workspace/_targets",
        "|| exit 0"), collapse = " "),
    name = "gcr.io/google.com/cloudsdktool/cloud-sdk:alpine",
    entrypoint = "bash",
    escape_dollar = FALSE,
    id = "get previous _targets metadata"
  )
}

#' @export
#' @rdname cr_buildstep_targets
cr_buildstep_targets_teardown <- function(bucket = cr_bucket_get()){
  c(
    cr_buildstep_bash(
      bash_script = paste(
        c(
          "date > buildtime.txt &&",
          "gsutil cp buildtime.txt",
          sprintf("%s/_targets/buildtime.txt", bucket)
        ), collapse = " "),
      name = "gcr.io/google.com/cloudsdktool/cloud-sdk:alpine",
      entrypoint = "bash",
      escape_dollar = FALSE,
      id = "Ensure bucket/_targets/ always exists"
    ),
    cr_buildstep_bash(
      bash_script = paste(
        c("gsutil -m cp -r", "/workspace/_targets", bucket,
          "&& gsutil ls -r", bucket),
        collapse = " "),
      name = "gcr.io/google.com/cloudsdktool/cloud-sdk:alpine",
      entrypoint = "bash",
      escape_dollar = FALSE,
      id = "Upload Artifacts"
    )
  )
}

#' Buildstep to run a targets pipeline on Cloud Build
#'
#' This is a buildstep to help upload a targets pipeline, see \link{cr_build_targets} for examples and suggested workflow
#' @export
#' @param task_args A named list of additional arguments to send to \link{cr_buildstep_r} when its executing the \link[targets]{tar_make} command (such as environment arguments or waitFor ids)
#' @param tar_make The R script that will run in the \code{tar_make()} step. Modify to include custom settings
#' @param task_image An existing Docker image that will be used to run your targets workflow after the targets meta has been downloaded from Google Cloud Storage
#' @family Cloud Buildsteps
cr_buildstep_targets <- function(
  task_args = list(),
  tar_make = "targets::tar_make()",
  task_image = "gcr.io/gcer-public/targets",
  id = "target pipeline"){

  do.call(
    cr_buildstep_r,
    args = c(
      task_args,
      list(
        r = tar_make,
        name = task_image,
        id = id
      )
    )
  )

}

#' @export
#' @rdname cr_buildstep_targets
#' @param bucket_folder The Google Cloud Storage bucket and folder the target metadata will be saved to, e.g. \code{gs://my-bucket/my_target_project}   You can also pass in build substitution variables such as \code{"${_MY_BUCKET}"}.
cr_buildstep_targets_setup <- function(bucket_folder){
  cr_buildstep_bash(
    bash_script = paste(
      c("mkdir /workspace/_targets &&",
        "mkdir /workspace/_targets/meta &&",
        "gsutil -m cp -r",
        sprintf("%s/_targets/meta",bucket_folder),
        "/workspace/_targets",
        "|| exit 0"), collapse = " "),
    name = "gcr.io/google.com/cloudsdktool/cloud-sdk:alpine",
    entrypoint = "bash",
    escape_dollar = FALSE,
    id = "get previous _targets metadata"
  )
}

#' @export
#' @rdname cr_buildstep_targets
#' @param last_id The final buildstep that needs to complete before the upload.  If left NULL then will default to the last tar_target step.
cr_buildstep_targets_teardown <- function(bucket_folder, last_id = NULL){
  cr_buildstep_bash(
    bash_script = paste(
      c(
        "date > buildtime.txt &&",
        "gsutil cp buildtime.txt",
        sprintf("%s/_targets/buildtime.txt", bucket_folder),
        "&& gsutil -m cp -r", "/workspace/_targets", bucket_folder,
        "&& gsutil ls -r", bucket_folder
      ),
      collapse = " "),
    name = "gcr.io/google.com/cloudsdktool/cloud-sdk:alpine",
    entrypoint = "bash",
    escape_dollar = FALSE,
    id = "Upload Artifacts",
    waitFor = last_id
  )
}

#' Create a DAG for Cloud Build from the targets pipeline
#' @inheritParams cr_buildstep_targets
#' @inheritParams cr_buildstep_targets_teardown
#' @rdname cr_build_targets
cr_buildstep_targets_multi <- function(
  target_folder = NULL,
  bucket = cr_bucket_get(),
  task_image = "gcr.io/gcer-public/targets",
  last_id = NULL
){

  target_bucket <- resolve_bucket_folder(target_folder,
                                         bucket)

  myMessage("Resolving targets::tar_manifest()", level = 3)
  nodes <- targets::tar_manifest()
  edges <- targets::tar_network()$edges

  first_id <- nodes$name[[1]]

  myMessage("# Building DAG:", level = 3)
  bst <- lapply(nodes$name, function(x){
    wait_for <- edges[edges$to == x,"from"][[1]]
    if(length(wait_for) == 0){
      wait_for <- NULL
    }

    if(x == first_id){
      wait_for <- "get previous _targets metadata"
    }

    myMessage("[",
              paste(wait_for, collapse = ", "),
              "] -> [", x, "]",
              level = 3)

    cr_buildstep_targets(
      task_args = list(
        waitFor = wait_for
      ),
      tar_make = sprintf("targets::tar_make('%s')", x),
      task_image = task_image,
      id = x
    )
  })

  bst <- unlist(bst, recursive = FALSE)

  if(is.null(last_id)){
    last_id <- nodes$name[[nrow(nodes)]]
  }
  last_id <- nodes$name[[nrow(nodes)]]

  c(
    cr_buildstep_targets_setup(target_bucket),
    bst,
    cr_buildstep_targets_teardown(target_bucket,
                                  last_id = last_id)
  )
}

