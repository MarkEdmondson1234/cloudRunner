% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tar_google_cloudbuild.R
\name{tar_google_cloudbuild}
\alias{tar_google_cloudbuild}
\alias{tar_google_trigger}
\title{Set up Google Cloud Build to run a targets pipeline}
\usage{
tar_google_cloudbuild(
  task_image = "gcr.io/gcer-public/targets",
  target_folder = basename(rstudioapi::getActiveProject()),
  path = "cloudbuild_targets.yaml",
  ask = NULL,
  bucket = googleCloudRunner::cr_bucket_get(),
  task_args = list(),
  ...
)

tar_google_trigger(trigger, path = "cloudbuild_targets.yaml")
}
\arguments{
\item{task_image}{An existing Docker image that will be used to run your targets workflow after the job state has been downloaded from Google Cloud Storage}

\item{target_folder}{Where target metadata will sit within the Google Cloud Storage bucket as a folder.  Defaults to RStudio project name.
tar_google_cloudbuild(tempfile())}

\item{path}{Character of length 1, file path to write the Google Cloud Build yaml
workflow file.}

\item{bucket}{The Google Cloud Storage bucket the target metadata will be saved to in folder `target_folder`}

\item{task_args}{A named list of additional arguments to send to `cr_buildstep_r()` when its executing the `targets::tar_make()` command (such as environment arguments)}

\item{...}{
  Arguments passed on to \code{\link[=cr_build_yaml]{cr_build_yaml}}
  \describe{
    \item{\code{steps}}{A vector of \link{cr_buildstep}}
    \item{\code{timeout}}{How long the entire build will run. If not set will be 10mins}
    \item{\code{logsBucket}}{Where logs are written.  If you don't set this field, Cloud Build will use a default bucket to store your build logs.}
    \item{\code{options}}{A named list of options}
    \item{\code{substitutions}}{Build macros that will replace entries in other elements}
    \item{\code{tags}}{Tags for the build}
    \item{\code{secrets}}{A secrets object}
    \item{\code{images}}{What images will be build from this cloudbuild}
    \item{\code{artifacts}}{What artifacts may be built from this cloudbuild - create via \link{cr_build_yaml_artifact}}
    \item{\code{availableSecrets}}{What environment arguments from Secret Manager are available to the build - create via \link{cr_build_yaml_secrets}}
    \item{\code{serviceAccount}}{What service account should the build be run under?}
  }}

\item{trigger}{A trigger for the build: a GitHub repo commit via \link[googleCloudRunner]{cr_buildtrigger_repo}; a PubSub trigger via \link[googleCloudRunner]{cr_buildtrigger_pubsub}; a webhook via \link[googleCloudRunner]{cr_buildtrigger_webhook}}
}
\value{
Nothing (invisibly). This function writes a Google Cloud Build yaml.
}
\description{
Writes a Google Cloud Build workflow file so the pipeline
  runs on every push to GitHub, or via a PubSub trigger (that could be scheduled). Historical runs accumulate in the
  configured Google Cloud Storage bucket, and the latest output is downloaded before
  [tar_make()] so up-to-date targets do not rerun.
}
\details{
Steps to set up your target task in Cloud Build:
  1. Create your `targets` workflow.
  1. Ensure your pipeline stays within the resource limitations of
    Google Cloud Build, both for storage and compute.
    For storage, you may wish to reduce the burden with
    GCP-backed storage formats like `"gcp_qs"`.
  1. Create a Dockerfile that holds the R and system dependencies for your workflow.  You can test the image using `googleCloudRunner::cr_deploy_docker()`.  Include `targets`.
  5. Run `tar_google_cloudbuild()` to create the cloudbuild yaml file.
  6. Create a build trigger via `tar_google_trigger`.  A common trigger is a GitHub push created via `googleCloudRunner::cr_buildtrigger_repo()`.  The first event after the trigger is created will run the pipeline, subsequent runs will only recompute the outdated targets.
  7. Inspect the Google Cloud Storage bucket you specified for the workflow artifacts.
}
\examples{
tar_google_cloudbuild(tempfile())
}
\concept{scripts}
