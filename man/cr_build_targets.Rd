% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/build_targets.R
\name{cr_build_targets}
\alias{cr_build_targets}
\alias{cr_build_targets_artifacts}
\title{Set up Google Cloud Build to run a targets pipeline}
\usage{
cr_build_targets(
  task_image = "gcr.io/gcer-public/targets",
  target_folder = basename(rstudioapi::getActiveProject()),
  path = "cloudbuild_targets.yaml",
  bucket = cr_bucket_get(),
  task_args = list(),
  tar_make = "targets::tar_make()",
  ...
)

cr_build_targets_artifacts(
  build,
  download_folder = "_targets_cloudbuild",
  target_subfolder = c("all", "meta", "objects", "user"),
  overwrite = TRUE
)
}
\arguments{
\item{task_image}{An existing Docker image that will be used to run your targets workflow after the targets meta has been downloaded from Google Cloud Storage}

\item{target_folder}{Where target metadata will sit within the Google Cloud Storage bucket as a folder.  Defaults to RStudio project name.}

\item{path}{File path to write the Google Cloud Build yaml workflow file. Set to NULL to write no file and just return the \link{Yaml} object.}

\item{bucket}{The Google Cloud Storage bucket the target metadata will be saved to in folder `target_folder`}

\item{task_args}{A named list of additional arguments to send to \link{cr_buildstep_r()} when its executing the \link[targets]{tar_make()} command (such as environment arguments)}

\item{tar_make}{The R script that will run in the tar_make() step. Modify to include custom settings such as "script"}

\item{...}{
  Arguments passed on to \code{\link[=cr_build_yaml]{cr_build_yaml}}
  \describe{
    \item{\code{steps}}{A vector of \link{cr_buildstep}}
    \item{\code{timeout}}{How long the entire build will run. If not set will be 10mins}
    \item{\code{logsBucket}}{Where logs are written.  If you don't set this field, Cloud Build will use a default bucket to store your build logs.}
    \item{\code{options}}{A named list of options}
    \item{\code{substitutions}}{Build macros that will replace entries in other elements}
    \item{\code{tags}}{Tags for the build}
    \item{\code{secrets}}{A secrets object}
    \item{\code{images}}{What images will be build from this cloudbuild}
    \item{\code{artifacts}}{What artifacts may be built from this cloudbuild - create via \link{cr_build_yaml_artifact}}
    \item{\code{availableSecrets}}{What environment arguments from Secret Manager are available to the build - create via \link{cr_build_yaml_secrets}}
    \item{\code{serviceAccount}}{What service account should the build be run under?}
  }}

\item{build}{A \link{Build} object that includes the artifact location}

\item{download_folder}{Where to download the artifact files}

\item{target_subfolder}{If you only want to download a specific folder from the _targets/ folder on Cloud Build then specify it here.}

\item{overwrite}{Whether to overwrite existing local data}
}
\value{
A Yaml object as generated by \link{cr_build_yaml}

\code{cr_build_targets_artifacts} returns the file path to where the download occurred.
}
\description{
Creates a Google Cloud Build yaml file so as to execute \link[targets]{tar_make} pipelines

Historical runs accumulate in the
  configured Google Cloud Storage bucket, and the latest output is downloaded before
  \link[targets]{tar_make} executes so up-to-date steps do not rerun.
}
\details{
Steps to set up your target task in Cloud Build:

\itemize{
  \item Create your `targets` workflow.
  \item Create a Dockerfile that holds the R and system dependencies for your workflow.  You can test the image using \link{cr_deploy_docker()}.  Include \code{library(targets)} dependencies - a Docker image with \code{targets} installed is available at \code{gcr.io/gcer-public/targets}.
  \item Run \code{cr_build_targets()} to create the cloudbuild yaml file.
  \item Run the build via \link{cr_build} or similar.  Each build should only recompute outdated targets.
  \item Optionally create a build trigger via \link{cr_buildtrigger}.
  \item Trigger a build. The first trigger will run the targets pipeline, subsequent runs will only recompute the outdated targets.
 }

Use \code{cr_build_targets_artifacts} to download the return values of a
  target Cloud Build, then \link[targets]{tar_read} to read the results.  You can set the downloaded files as the target store via \code{targets::tar_config_set(store="_targets_cloudbuild")}.  Set \code{download_folder = "_targets"} to overwrite your local targets store.
}
\examples{

cr_build_targets(path = tempfile())

# adding custom environment args and secrets to the build
cr_build_targets(
  task_image = "gcr.io/my-project/my-targets-pipeline",
  options = list(env = c(
    "ENV1=1234",
    "ENV_USER=Dave"
  )),
  availableSecrets = cr_build_yaml_secrets("MY_PW", "my-pw"),
  task_args = list(secretEnv = "MY_PW")
)
}
\seealso{
Other Cloud Build functions: 
\code{\link{Build}()},
\code{\link{RepoSource}()},
\code{\link{Source}()},
\code{\link{StorageSource}()},
\code{\link{cr_build_artifacts}()},
\code{\link{cr_build_list}()},
\code{\link{cr_build_logs}()},
\code{\link{cr_build_make}()},
\code{\link{cr_build_status}()},
\code{\link{cr_build_upload_gcs}()},
\code{\link{cr_build_wait}()},
\code{\link{cr_build_write}()},
\code{\link{cr_build_yaml_artifact}()},
\code{\link{cr_build_yaml_secrets}()},
\code{\link{cr_build_yaml}()},
\code{\link{cr_build}()}
}
\concept{Cloud Build functions}
